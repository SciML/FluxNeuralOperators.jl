export
    OperatorConv,
    SpectralConv,
    OperatorKernel,
    GraphKernel

struct OperatorConv{P, T, S, TT}
    weight::T
    in_channel::S
    out_channel::S
    transform::TT
end

function OperatorConv{P}(
    weight::T,
    in_channel::S,
    out_channel::S,
    transform::TT
) where {P, T, S, TT<:AbstractTransform}
    return OperatorConv{P, T, S, TT}(weight, in_channel, out_channel, transform)
end

"""
    OperatorConv(
        ch, modes, transform;
        init=c_glorot_uniform, permuted=false, T=ComplexF32
    )

## Arguments

* `ch`: Input and output channel size, e.g. `64=>64`.
* `modes`: The modes to be preserved.
* `Transform`: The trafo to operate the transformation.
* `permuted`: Whether the dim is permuted. If `permuted=true`, layer accepts
    data in the order of `(ch, ..., batch)`, otherwise the order is `(..., ch, batch)`.

## Example

```jldoctest
julia> OperatorConv(2=>5, (16, ), FourierTransform)
OperatorConv(2 => 5, (16,), FourierTransform, permuted=false)

julia> OperatorConv(2=>5, (16, ), FourierTransform, permuted=true)
OperatorConv(2 => 5, (16,), FourierTransform, permuted=true)
```
"""
function OperatorConv(
    ch::Pair{S, S},
    modes::NTuple{N, S},
    Transform::Type{<:AbstractTransform};
    init=c_glorot_uniform,
    permuted=false,
    T::DataType=ComplexF32
) where {S<:Integer, N}
    in_chs, out_chs = ch
    scale = one(T) / (in_chs * out_chs)
    weights = scale * init(prod(modes), in_chs, out_chs)
    transform = Transform(modes)

    return OperatorConv{permuted}(weights, in_chs, out_chs, transform)
end

function SpectralConv(
    ch::Pair{S, S},
    modes::NTuple{N, S};
    init=c_glorot_uniform,
    permuted=false,
    T::DataType=ComplexF32
) where {S<:Integer, N}
    return OperatorConv(ch, modes, FourierTransform, init=init, permuted=permuted, T=T)
end

Flux.@functor OperatorConv{true}
Flux.@functor OperatorConv{false}

Base.ndims(oc::OperatorConv) = ndims(oc.transform)

ispermuted(::OperatorConv{P}) where {P} = P

function Base.show(io::IO, l::OperatorConv{P}) where {P}
    print(io, "OperatorConv($(l.in_channel) => $(l.out_channel), $(l.transform.modes), $(nameof(typeof(l.transform))), permuted=$P)")
end

function operator_conv(m::OperatorConv, ğ±::AbstractArray)
    ğ±_transformed = transform(m.transform, ğ±) # [size(x)..., in_chs, batch]
    ğ±_truncated = truncate_modes(m.transform, ğ±_transformed) # [modes..., in_chs, batch]
    ğ±_applied_pattern = apply_pattern(ğ±_truncated, m.weight) # [modes..., out_chs, batch]
    ğ±_padded = pad_modes(ğ±_applied_pattern, (size(ğ±_transformed)[1:end-2]..., size(ğ±_applied_pattern)[end-1:end]...)) # [size(x)..., out_chs, batch] <- [modes..., out_chs, batch]
    ğ±_inversed = inverse(m.transform, ğ±_padded)

    return ğ±_inversed
end

function (m::OperatorConv{false})(ğ±)
    ğ±áµ€ = permutedims(ğ±, (ntuple(i->i+1, ndims(m))..., 1, ndims(m)+2)) # [x, in_chs, batch] <- [in_chs, x, batch]
    ğ±_out = operator_conv(m, ğ±áµ€) # [x, out_chs, batch]
    ğ±_outáµ€ = permutedims(ğ±_out, (ndims(m)+1, 1:ndims(m)..., ndims(m)+2)) # [out_chs, x, batch] <- [x, out_chs, batch]

    return ğ±_outáµ€
end

function (m::OperatorConv{true})(ğ±)
    return operator_conv(m, ğ±) # [x, out_chs, batch]
end

############
# operator #
############

struct OperatorKernel{L, C, F}
    linear::L
    conv::C
    Ïƒ::F
end

"""
    OperatorKernel(ch, modes, Ïƒ=identity; permuted=false)

## Arguments

* `ch`: Input and output channel size for spectral convolution, e.g. `64=>64`.
* `modes`: The Fourier modes to be preserved for spectral convolution.
* `Ïƒ`: Activation function.
* `permuted`: Whether the dim is permuted. If `permuted=true`, layer accepts
    data in the order of `(ch, ..., batch)`, otherwise the order is `(..., ch, batch)`.

## Example

```jldoctest
julia> OperatorKernel(2=>5, (16, ), FourierTransform)
OperatorKernel(2 => 5, (16,), FourierTransform, Ïƒ=identity, permuted=false)

julia> using Flux

julia> OperatorKernel(2=>5, (16, ), FourierTransform, relu)
OperatorKernel(2 => 5, (16,), FourierTransform, Ïƒ=relu, permuted=false)

julia> OperatorKernel(2=>5, (16, ), FourierTransform, relu, permuted=true)
OperatorKernel(2 => 5, (16,), FourierTransform, Ïƒ=relu, permuted=true)
```
"""
function OperatorKernel(
    ch::Pair{S, S},
    modes::NTuple{N, S},
    Transform::Type{<:AbstractTransform},
    Ïƒ=identity;
    permuted=false
) where {S<:Integer, N}
    linear = permuted ? Conv(Tuple(ones(Int, length(modes))), ch) : Dense(ch.first, ch.second)
    conv = OperatorConv(ch, modes, Transform; permuted=permuted)

    return OperatorKernel(linear, conv, Ïƒ)
end

Flux.@functor OperatorKernel

function Base.show(io::IO, l::OperatorKernel)
    print(
        io,
        "OperatorKernel(" *
            "$(l.conv.in_channel) => $(l.conv.out_channel), " *
            "$(l.conv.transform.modes), " *
            "$(nameof(typeof(l.conv.transform))), " *
            "Ïƒ=$(string(l.Ïƒ)), " *
            "permuted=$(ispermuted(l.conv))" *
        ")"
    )
end

function (m::OperatorKernel)(ğ±)
    return m.Ïƒ.(m.linear(ğ±) + m.conv(ğ±))
end

"""
    GraphKernel(Îº, ch, Ïƒ=identity)

Graph kernel layer.

## Arguments

* `Îº`: A neural network layer for approximation, e.g. a `Dense` layer or a MLP.
* `ch`: Channel size for linear transform, e.g. `32`.
* `Ïƒ`: Activation function.
"""
struct GraphKernel{A,B,F} <: MessagePassing
    linear::A
    Îº::B
    Ïƒ::F
end

function GraphKernel(Îº, ch::Int, Ïƒ=identity; init=Flux.glorot_uniform)
    W = init(ch, ch)
    return GraphKernel(W, Îº, Ïƒ)
end

Flux.@functor GraphKernel

function GeometricFlux.message(l::GraphKernel, x_i::AbstractArray, x_j::AbstractArray, e_ij)
    return l.Îº(vcat(x_i, x_j))
end

function GeometricFlux.update(l::GraphKernel, m::AbstractArray, x::AbstractArray)
    return l.Ïƒ.(GeometricFlux._matmul(l.linear, x) + m)
end

function (l::GraphKernel)(el::NamedTuple, X::AbstractArray)
    GraphSignals.check_num_nodes(el.N, X)
    _, V, _ = GeometricFlux.propagate(l, el, nothing, X, nothing, mean, nothing, nothing)
    return V
end

function Base.show(io::IO, l::GraphKernel)
    channel, _ = size(l.linear)
    print(io, "GraphKernel(", l.Îº, ", channel=", channel)
    l.Ïƒ == identity || print(io, ", ", l.Ïƒ)
    print(io, ")")
end


#########
# utils #
#########

c_glorot_uniform(dims...) = Flux.glorot_uniform(dims...) + Flux.glorot_uniform(dims...)*im

# [prod(modes), out_chs, batch] <- [prod(modes), in_chs, batch] * [out_chs, in_chs, prod(modes)]
einsum(ğ±â‚, ğ±â‚‚) = @tullio ğ²[m, o, b] := ğ±â‚[m, i, b] * ğ±â‚‚[m, i, o]

function apply_pattern(ğ±_truncated, ğ°)
    x_size = size(ğ±_truncated) # [m.modes..., in_chs, batch]

    ğ±_flattened = reshape(ğ±_truncated, :, x_size[end-1:end]...) # [prod(m.modes), in_chs, batch], only 3-dims
    ğ±_weighted = einsum(ğ±_flattened, ğ°) # [prod(m.modes), out_chs, batch], only 3-dims
    ğ±_shaped = reshape(ğ±_weighted, x_size[1:end-2]..., size(ğ±_weighted)[2:3]...) # [m.modes..., out_chs, batch]

    return ğ±_shaped
end

pad_modes(ğ±::AbstractArray, dims::NTuple) = pad_modes!(similar(ğ±, dims), ğ±)

function pad_modes!(ğ±_padded::AbstractArray, ğ±::AbstractArray)
    fill!(ğ±_padded, eltype(ğ±)(0)) # zeros(eltype(ğ±), dims)
    ğ±_padded[map(d->1:d, size(ğ±))...] .= ğ±

    return ğ±_padded
end

function ChainRulesCore.rrule(::typeof(pad_modes), ğ±::AbstractArray, dims::NTuple)
    function pad_modes_pullback(ğ²Ì„)
        return NoTangent(), view(ğ²Ì„, map(d->1:d, size(ğ±))...), NoTangent()
    end

    return pad_modes(ğ±, dims), pad_modes_pullback
end
